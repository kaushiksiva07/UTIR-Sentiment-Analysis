{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('aclImdb/train/trainReviews.txt', 'r', encoding=\"utf8\") as f:\n",
    "     reviews_train = f.read()\n",
    "labels_train = pd.read_csv('aclImdb/train/IMDBValuesTrain.txt', sep='delimiter', names=['Score'], engine = 'python')\n",
    "\n",
    "\n",
    "# tockenizing training data \n",
    "reviews_train = reviews_train.lower()\n",
    "\n",
    "all_text_train = ''.join([c for c in reviews_train if c not in punctuation])\n",
    "\n",
    "reviews_split_train = all_text_train.split('\\n')\n",
    "\n",
    "labels_train = labels_train.to_numpy()\n",
    "\n",
    "all_text2_train = ' '.join(reviews_split_train)\n",
    "\n",
    "words_train = all_text2_train.split()\n",
    "\n",
    "count_words_train = Counter(words_train)\n",
    "\n",
    "total_words_train = len(words_train)\n",
    "sorted_words_train = count_words_train.most_common(total_words_train)\n",
    "\n",
    "vocab_to_int_train = {w:i+1 for i, (w,c) in enumerate(sorted_words_train)}\n",
    "\n",
    "\n",
    "# Change Review Score to Binary Score\n",
    "encoded_labels_train = [0 if label <= 5 else 1 for label in labels_train]\n",
    "encoded_labels_train = np.array(encoded_labels_train)\n",
    "\n",
    "reviews_int_train = []\n",
    "for reviews in reviews_split_train:\n",
    "    r = [vocab_to_int_train[w] for w in reviews.split()]\n",
    "    reviews_int_train.append(r)\n",
    "    \n",
    "\n",
    "reviews_len_train = [len(x) for x in reviews_int_train]\n",
    "reviews_int_train = [reviews_int_train[i] for i, l in enumerate(reviews_len_train) if l > 0]   #do we need this? \n",
    "encoded_labels_train = [encoded_labels_train[i] for i, l in enumerate(reviews_len_train) if l > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('aclImdb/test/testReviews.txt', 'r', encoding=\"utf8\") as f:\n",
    "    reviews_test = f.read()\n",
    "labels_test = pd.read_csv('aclImdb/test/IMDBValuesTest.txt', sep='delimiter', names=['Score'], engine = 'python')\n",
    "\n",
    "\n",
    "# tockenizing test data \n",
    "reviews_test = reviews_test.lower()\n",
    "\n",
    "all_text_test = ''.join([c for c in reviews_test if c not in punctuation])\n",
    "\n",
    "reviews_split_test = all_text_test.split('\\n')\n",
    "\n",
    "labels_test = labels_test.to_numpy()\n",
    "\n",
    "all_text2_test = ' '.join(reviews_split_test)\n",
    "\n",
    "words_test = all_text2_test.split()\n",
    "\n",
    "count_words_test = Counter(words_test)\n",
    "\n",
    "total_words_test = len(words_test)\n",
    "sorted_words_test = count_words_test.most_common(total_words_test)\n",
    "\n",
    "vocab_to_int_test = {w:i+1 for i, (w,c) in enumerate(sorted_words_test)}\n",
    "\n",
    "\n",
    "# Change Review Score to Binary Score\n",
    "encoded_labels_test = [0 if label <= 5 else 1 for label in labels_test]\n",
    "encoded_labels_test = np.array(encoded_labels_test)\n",
    "\n",
    "reviews_int_test = []\n",
    "for reviews in reviews_split_test:\n",
    "    r = [vocab_to_int_test[w] for w in reviews.split()]\n",
    "    reviews_int_test.append(r)\n",
    "    \n",
    "\n",
    "reviews_len_test = [len(x) for x in reviews_int_test]\n",
    "reviews_int_test = [reviews_int_test[i] for i, l in enumerate(reviews_len_test) if l > 0]   # remove outliers\n",
    "encoded_labels_test = [encoded_labels_test[i] for i, l in enumerate(reviews_len_test) if l > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 200\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features\n",
    "\n",
    "features_train = pad_features(reviews_int_train, seq_length)\n",
    "features_test = pad_features(reviews_int_test, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.9\n",
    "len_feat = len(features_test)\n",
    "\n",
    "remaining_x = features_test[int(split_frac*len_feat):]\n",
    "remaining_y = encoded_labels_test[int(split_frac*len_feat):]\n",
    "\n",
    "valid_x = np.array(remaining_x[0:int(len(remaining_x)*0.5)])\n",
    "valid_y = np.array(remaining_y[0:int(len(remaining_y)*0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,   625,     1,  1434],\n",
      "        [    0,     0,     0,  ..., 13198,    12,  1816],\n",
      "        [    3,   579,     4,  ..., 48030,  2596,  1754],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,    57,    64, 13732],\n",
      "        [    0,     0,     0,  ..., 28229, 55287, 43161],\n",
      "        [    0,     0,     0,  ...,     3,  1378,  3824]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "\n",
    "# convert features and encoded_labels to np array for torch library \n",
    "features_train = np.array(features_train); encoded_labels_train = np.array(encoded_labels_train)\n",
    "features_test = np.array(features_test); encoded_labels_test = np.array(encoded_labels_test)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(features_train), torch.from_numpy(encoded_labels_train))\n",
    "test_data = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(encoded_labels_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "\n",
    "# Dataloaders - Shuffle Data\n",
    "batch_size = 50\n",
    "                           \n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int_train)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers) # call our RNN pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.655830... Val Loss: 0.597463\n",
      "Epoch: 1/3... Step: 200... Loss: 0.719412... Val Loss: 0.468063\n",
      "Epoch: 1/3... Step: 300... Loss: 0.765935... Val Loss: 1.267863\n",
      "Epoch: 1/3... Step: 400... Loss: 0.752900... Val Loss: 1.212787\n",
      "Epoch: 1/3... Step: 500... Loss: 0.360630... Val Loss: 0.296609\n",
      "Epoch: 2/3... Step: 600... Loss: 0.543922... Val Loss: 1.133109\n",
      "Epoch: 2/3... Step: 700... Loss: 0.610358... Val Loss: 0.425605\n",
      "Epoch: 2/3... Step: 800... Loss: 0.328867... Val Loss: 0.420704\n",
      "Epoch: 2/3... Step: 900... Loss: 0.520030... Val Loss: 1.077079\n",
      "Epoch: 2/3... Step: 1000... Loss: 0.322840... Val Loss: 0.974163\n",
      "Epoch: 3/3... Step: 1100... Loss: 0.198299... Val Loss: 0.960107\n",
      "Epoch: 3/3... Step: 1200... Loss: 0.195233... Val Loss: 1.189246\n",
      "Epoch: 3/3... Step: 1300... Loss: 0.329660... Val Loss: 1.522535\n",
      "Epoch: 3/3... Step: 1400... Loss: 0.237844... Val Loss: 1.249859\n",
      "Epoch: 3/3... Step: 1500... Loss: 0.393153... Val Loss: 1.536370\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "    device = \"cuda:0\"\n",
    "    net = net.to(device)\n",
    "\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        inputs = inputs.to(device)\n",
    "        output, h = net(inputs, h)\n",
    "        output = output.to(device)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                inputs = inputs.to(device)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                output = output.to(device)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.048\n",
      "Test accuracy: 0.541\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    inputs = inputs.to(device)\n",
    "    output, h = net(inputs, h)\n",
    "    output = output.to(device)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
