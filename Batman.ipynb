{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial found at: https://courses.engr.illinois.edu/ie534/fa2018/NLP.html#part-1-bag-of-words\n",
    "from torch.autograd import Variable\n",
    "from BOW_model import BOW_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import itertools\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "\n",
    "class BOW_model(nn.Module):\n",
    "    def __init__(self, vocab_size, no_of_hidden_units):\n",
    "        super(BOW_model, self).__init__()\n",
    "        ## will need to define model architecture\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # will need to define forward function for when model gets called\n",
    "        \n",
    "    def __init__(self, vocab_size, no_of_hidden_units):\n",
    "        super(BOW_model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,no_of_hidden_units)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(no_of_hidden_units,no_of_hidden_units)\n",
    "        self.bn_hidden = nn.BatchNorm1d(no_of_hidden_units)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_output = nn.Linear(no_of_hidden_units, 1)\n",
    "        \n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "    def forward(self, x, t):\n",
    "    \n",
    "        bow_embedding = []\n",
    "        for i in range(len(x)):\n",
    "            lookup_tensor = Variable(torch.LongTensor(x[i])).cuda()\n",
    "            embed = self.embedding(lookup_tensor)\n",
    "            embed = embed.mean(dim=0)\n",
    "            bow_embedding.append(embed)\n",
    "        bow_embedding = torch.stack(bow_embedding)\n",
    "    \n",
    "        h = self.dropout(F.relu(self.bn_hidden(self.fc_hidden(bow_embedding))))\n",
    "        h = self.fc_output(h)\n",
    "    \n",
    "        return self.loss(h[:,0],t), h[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing data\n",
    "\n",
    "#my_tar = tarfile.open('C:/Users/huang/Documents/pythonUTIR_NLP/UTIR-Sentiment-Analysis/aclImdb_v1.tar.gz')\n",
    "#my_tar.extractall('./my_folder') # specify which folder to extract to\n",
    "#my_tar.close()\n",
    "\n",
    "## create directory to store preprocessed data\n",
    "if(not os.path.isdir('preprocessed_data')):\n",
    "    os.mkdir('preprocessed_data')\n",
    "    \n",
    "train_directory = 'C:/Users/huang/Documents/pythonUTIR_NLP/UTIR-Sentiment-Analysis/aclImdb/train'\n",
    "pos_filenames = os.listdir(train_directory + '/pos/')\n",
    "\n",
    "neg_filenames = os.listdir(train_directory + '/neg/')\n",
    "unsup_filenames = os.listdir(train_directory + '/unsup/')\n",
    "\n",
    "pos_filenames = [train_directory+'/pos/'+filename for filename in pos_filenames]\n",
    "neg_filenames = [train_directory+'/neg/'+filename for filename in neg_filenames]\n",
    "unsup_filenames = [train_directory+'/unsup/'+filename for filename in unsup_filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_directory = 'C:/Users/kaush/Desktop/aclImdb/train'\n",
    "# pos_filenames = os.listdir(train_directory + '/pos/')\n",
    "\n",
    "# neg_filenames = os.listdir(train_directory + '/neg/')\n",
    "# unsup_filenames = os.listdir(train_directory + '/unsup/')\n",
    "\n",
    "# pos_filenames = [train_directory+'/pos/'+filename for filename in pos_filenames]\n",
    "# neg_filenames = [train_directory+'/neg/'+filename for filename in neg_filenames]\n",
    "# unsup_filenames = [train_directory+'/unsup/'+filename for filename in unsup_filenames]\n",
    "\n",
    "filenames = pos_filenames + neg_filenames + unsup_filenames\n",
    "\n",
    "count = 0\n",
    "x_train = []\n",
    "for filename in filenames:\n",
    "    with io.open(filename,'r',encoding='cp437') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "\n",
    "    x_train.append(line)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all of the test reviews\n",
    "test_directory = 'C:/Users/huang/Documents/pythonUTIR_NLP/UTIR-Sentiment-Analysis/aclImdb/test'\n",
    "#test_directory = 'C:/Users/kaush/Desktop/aclImdb/test'\n",
    "\n",
    "pos_filenames = os.listdir(test_directory + '/pos/')\n",
    "neg_filenames = os.listdir(test_directory + '/neg/')\n",
    "\n",
    "pos_filenames = [test_directory+'/pos/'+filename for filename in pos_filenames]\n",
    "neg_filenames = [test_directory+'/neg/'+filename for filename in neg_filenames]\n",
    "\n",
    "filenames = pos_filenames+neg_filenames\n",
    "\n",
    "count = 0\n",
    "x_test = []\n",
    "for filename in filenames:\n",
    "    with io.open(filename,'r',encoding='utf-8') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "\n",
    "    x_test.append(line)\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  997807  Min:  26  Max:  2738  Mean:  273.7467764060357  Std:  210.76569615609978\n"
     ]
    }
   ],
   "source": [
    "## number of tokens per review\n",
    "no_of_tokens = []\n",
    "for tokens in x_train:\n",
    "    no_of_tokens.append(len(tokens))\n",
    "no_of_tokens = np.asarray(no_of_tokens)\n",
    "print('Total: ', np.sum(no_of_tokens), ' Min: ', np.min(no_of_tokens), ' Max: ', np.max(no_of_tokens), ' Mean: ', np.mean(no_of_tokens), ' Std: ', np.std(no_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word_to_id and id_to_word. associate an id to every unique token in the training data\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
    "\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([160758,  26834,   7815,   1372], dtype=int64), array([    1,    10,   100,  1000, 10000]))\n",
      "the 1009642.0\n",
      ", 829575.0\n",
      ". 817078.0\n",
      "and 492150.0\n",
      "a 488701.0\n",
      "of 438552.0\n",
      "to 405684.0\n",
      "is 330975.0\n",
      "it 285939.0\n",
      "in 280663.0\n"
     ]
    }
   ],
   "source": [
    "## let's sort the indices by word frequency instead of random\n",
    "x_train_token_ids = [[word_to_id[token] for token in x] for x in x_train]\n",
    "count = np.zeros(id_to_word.shape)\n",
    "for x in x_train_token_ids:\n",
    "    for token in x:\n",
    "        count[token] += 1\n",
    "indices = np.argsort(-count)\n",
    "id_to_word = id_to_word[indices]\n",
    "count = count[indices]\n",
    "\n",
    "hist = np.histogram(count,bins=[1,10,100,1000,10000]); #print(hist)\n",
    "# for i in range(10):\n",
    "#     print(id_to_word[i],count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recreate word_to_id based on sorted list\n",
    "word_to_id = {token: idx for idx, token in enumerate(id_to_word)}\n",
    "\n",
    "## assign -1 if token doesn't appear in our dictionary\n",
    "## add +1 to all token ids, we went to reserve id=0 for an unknown token\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dictionary\n",
    "np.save('preprocessed_data/imdb_dictionary.npy',np.asarray(id_to_word))\n",
    "\n",
    "## save training data to single text file\n",
    "with io.open('preprocessed_data/imdb_train.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_train_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "## save test data to single text file\n",
    "with io.open('preprocessed_data/imdb_test.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_test_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = 'C:/Users/huang/Documents/pythonUTIR_NLP/UTIR-Sentiment-Analysis/glove.840B.300d.txt'\n",
    "#glove_filename = 'C:/Users/kaush/Glove/glove.840B.300d.txt'\n",
    "\n",
    "with io.open(glove_filename,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "glove_dictionary = []\n",
    "glove_embeddings = []\n",
    "count = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    glove_dictionary.append(line[0])\n",
    "    embedding = np.asarray(line[1:],dtype=np.float)\n",
    "    glove_embeddings.append(embedding)\n",
    "    count+=1\n",
    "    if(count>=100000):\n",
    "        break\n",
    "\n",
    "glove_dictionary = np.asarray(glove_dictionary)\n",
    "glove_embeddings = np.asarray(glove_embeddings)\n",
    "# added a vector of zeros for the unknown tokens\n",
    "glove_embeddings = np.concatenate((np.zeros((1,300)),glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {token: idx for idx, token in enumerate(glove_dictionary)}\n",
    "\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preprocessed_data/glove_dictionary.npy',glove_dictionary)\n",
    "np.save('preprocessed_data/glove_embeddings.npy',glove_embeddings)\n",
    "\n",
    "with io.open('preprocessed_data/imdb_train_glove.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_train_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with io.open('preprocessed_data/imdb_test_glove.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_test_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'bow model'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
